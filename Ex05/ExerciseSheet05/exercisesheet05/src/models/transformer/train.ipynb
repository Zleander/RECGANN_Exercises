{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python3"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Main file for training a model<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import torch as th\n", "import torch.nn as nn\n", "import glob\n", "import os\n", "import sys\n", "import time\n", "import matplotlib.pyplot as plt\n", "from threading import Thread"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sys.path.append(\"../../utils\")\n", "from modules import Model\n", "from configuration import Configuration\n", "import helper_functions as helpers\n", "from tqdm import tqdm"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["_author_ = \"Matthias Karlbauer\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run_training():\n\n", "    # Load the user configurations\n", "    cfg = Configuration(r\"D:\\Uni Master\\Semester 2\\Recurrent and Generative NNs\\Assignments\\RECGANN_Exercises\\Ex05\\ExerciseSheet05\\exercisesheet05\\src\\models\\transformer\\config.json\")\n\n", "    # Print some information to console\n", "    print(\"Model name:\", cfg.model.name)\n\n", "    # Hide the GPU(s) in case the user specified to use the CPU in the config\n", "    # file\n", "    if cfg.general.device == \"CPU\":\n", "        os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n", "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n", "    # Set device on GPU if specified in the configuration file, else CPU\n", "    device = helpers.determine_device()\n", "    # Initialize and set up the model\n", "    model = Model(\n", "        n_heads=cfg.model.n_heads,\n", "        linear_layer_size=cfg.model.linear_layer_size,\n", "        d_model=cfg.model.d_model,\n", "        d_one_hot=cfg.model.d_one_hot,\n", "        num_blocks = cfg.model.num_blocks\n", "    ).to(device=device)\n", "    for p in model.parameters():\n", "        if p.dim() > 1:\n", "            nn.init.xavier_uniform_(p)\n\n", "    # Count number of trainable parameters\n", "    pytorch_total_params = sum(\n", "        p.numel() for p in model.parameters() if p.requires_grad\n", "    )\n", "    print(\"Trainable model parameters:\", pytorch_total_params)\n\n", "    #\n", "    # Set up an optimizer and the criterion (loss)\n", "    optimizer = th.optim.Adam(model.parameters(),\n", "                              lr=cfg.training.learning_rate,\n", "                              betas=(0.9, 0.98),\n", "                              eps=1e-9)\n", "                              \n", "    criterion = nn.CrossEntropyLoss()\n", "    # Set up a learning rate scheduler if desired\n", "    if cfg.training.scheduler == True:\n", "        scheduler = th.optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer, T_max = cfg.training.epochs)\n", "    #\n", "    # Set up lists to save and store the epoch errors\n", "    epoch_errors_train = []\n", "    best_train = np.infty\n\n", "    #\n", "    # Set up the dataloader\n", "    dataset, dataloader = helpers.build_dataloader(\n", "        cfg=cfg, batch_size=1\n", "    )\n", "    \"\"\"\n", "    TRAINING\n", "    \"\"\"\n", "    a = time.time()\n\n", "    #\n", "    # Start the training and iterate over all epochs\n", "    for epoch in range(cfg.training.epochs):\n", "        epoch_start_time = time.time()\n\n", "        # List to store the errors for each sequence\n", "        sequence_errors = []\n\n", "        # Iterate over the training batches\n", "        for batch_idx, (net_input, net_label) in tqdm(enumerate(dataloader)):\n\n", "            # Move data to the desired device and convert from\n", "            # [batch_size, time, dim] to [time, batch_size, dim]\n", "            net_input = net_input.to(device=device).transpose(0, 1)\n", "            net_label = net_label.to(device=device).transpose(0, 1)\n", "            \n", "            # Reset optimizer to clear the previous batch\n", "            optimizer.zero_grad()\n\n", "            # TODO: Generate prediction\n", "            y_hat = model(net_input.squeeze())\n\n", "            # Convert target one hot to indices (required for CE-loss)\n", "            # target = net_label[:, 0].data.topk(1)[1][:, 0]\n", "            loss = criterion(y_hat.squeeze(), net_label.squeeze())\n", "            loss.backward()\n", "            optimizer.step()\n", "            sequence_errors.append(loss.item())\n", "            \n", "        #After each epoch, use the LR-scheduler if it is set\n", "        if cfg.training.scheduler == True:\n", "            scheduler.step()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        epoch_errors_train.append(np.mean(sequence_errors))\n\n", "        # Save the model to file (if desired)\n", "        if cfg.training.save_model and np.mean(sequence_errors) < best_train:\n", "            # Start a separate thread to save the model\n", "            thread = Thread(target=helpers.save_model_to_file(\n", "                model_src_path=os.path.abspath(\"\"),\n", "                cfg=cfg,\n", "                epoch=epoch,\n", "                epoch_errors_train=epoch_errors_train,\n", "                model=model))\n", "            thread.start()\n\n", "        # Create a plus or minus sign for the training error\n", "        train_sign = \"(-)\"\n", "        if epoch_errors_train[-1] < best_train:\n", "            train_sign = \"(+)\"\n", "            best_train = epoch_errors_train[-1]\n", "        \n", "        #\n", "        # Print progress to the console\n", "        print(\n", "            f\"Epoch {str(epoch+1).zfill(int(np.log10(cfg.training.epochs))+1)}\"\n", "            f\"/{str(cfg.training.epochs)} took \"\n", "            f\"{str(np.round(time.time() - epoch_start_time, 2)).ljust(5, '0')} \"\n", "            f\"seconds. \\t\\tAverage epoch training error: \"\n", "            f\"{train_sign}\"\n", "            f\"{str(np.round(epoch_errors_train[-1], 10)).ljust(12, ' ')}\"\n", "        )\n", "    b = time.time()\n", "    print('\\nTraining took ' + str(np.round(b - a, 2)) + ' seconds.\\n\\n')"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}