{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python3"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Main file for training a model<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import torch as th\n", "import torch.nn as nn\n", "import glob\n", "import os\n", "import sys\n", "import time\n", "import matplotlib.pyplot as plt\n", "from threading import Thread"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sys.path.append(\"../../utils\")\n", "from modules import Model\n", "from configuration import Configuration\n", "import helper_functions as helpers\n", "from tqdm import tqdm"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["_author_ = \"Matthias Karlbauer\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run_training():\n\n", "    # Load the user configurations\n", "    cfg = Configuration(\"config.json\")\n\n", "    # Print some information to console\n", "    #print(\"Architecture name:\", cfg.model.architecture)\n", "    print(\"Model name:\", cfg.model.name)\n\n", "    # Hide the GPU(s) in case the user specified to use the CPU in the config\n", "    # file\n", "    if cfg.general.device == \"CPU\":\n", "        os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n", "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n", "    # Set device on GPU if specified in the configuration file, else CPU\n", "    device = helpers.determine_device()\n", "    \n", "    # Initialize and set up the model\n", "    model = Model(\n", "        d_one_hot=cfg.model.d_one_hot,\n", "        d_lstm=cfg.model.d_lstm,\n", "        num_lstm_layers=cfg.model.num_lstm_layers,\n", "        dropout=0\n", "    ).to(device=device)\n\n", "    # Count number of trainable parameters\n", "    pytorch_total_params = sum(\n", "        p.numel() for p in model.parameters() if p.requires_grad\n", "    )\n", "    print(\"Trainable model parameters:\", pytorch_total_params)\n\n", "    #\n", "    # Set up an optimizer and the criterion (loss)\n", "    optimizer = th.optim.Adam(model.parameters(),\n", "                              lr=cfg.training.learning_rate)\n", "    criterion = nn.CrossEntropyLoss()\n\n", "    #\n", "    # Set up a list to save and store the epoch errors\n", "    epoch_errors = []\n", "    best_error = np.infty\n\n", "    #\n", "    # Set up the training dataloader\n", "    dataset, dataloader = helpers.build_dataloader(\n", "        cfg=cfg, batch_size=1\n", "    )\n", "    \"\"\"\n", "    TRAINING\n", "    \"\"\"\n", "    a = time.time()\n\n", "    #\n", "    # Start the training and iterate over all epochs\n", "    for epoch in range(cfg.training.epochs):\n", "        epoch_start_time = time.time()\n\n", "        # List to store the errors for each sequence\n", "        sequence_errors = []\n\n", "        # Iterate over the training batches\n", "        for batch_idx, (net_input, net_label) in tqdm(enumerate(dataloader)):\n", "            #print(batch_idx, len(dataloader))\n\n", "            # Move data to the desired device and convert from\n", "            # [batch_size, time, dim] to [time, batch_size, dim]\n", "            net_input = net_input.to(device=device).transpose(0, 1)\n", "            net_label = net_label.to(device=device).transpose(0, 1)\n\n", "            # Reset optimizer to clear the previous batch\n", "            optimizer.zero_grad()\n\n", "            # DONE: Generate a model prediction\n", "            y_hat = model(net_input)\n", "            target = net_label\n", "            #print(y_hat[0].shape)\n", "            #print(target.shape)\n", "            loss = criterion(y_hat[0].squeeze(), target.squeeze())\n", "            # Compute gradients\n", "            loss.backward()\n\n", "            # Perform weight update\n", "            optimizer.step()\n", "            sequence_errors.append(loss.item())\n", "        print(f'Final error of epoch {np.mean(sequence_errors)}')\n", "        epoch_errors.append(np.mean(sequence_errors))\n\n", "        # Save the model to file (if desired)\n", "        if cfg.training.save_model and np.mean(sequence_errors) < best_error:\n", "            # Start a separate thread to save the model\n", "            thread = Thread(target=helpers.save_model_to_file(\n", "                model_src_path=os.path.abspath(\"\"),\n", "                cfg=cfg,\n", "                epoch=epoch,\n", "                epoch_errors_train=epoch_errors,\n", "                model=model))\n", "            thread.start()\n\n", "        # Create a plus or minus sign for the training error\n", "        train_sign = \"(-)\"\n", "        if epoch_errors[-1] < best_error:\n", "            train_sign = \"(+)\"\n", "            best_error = epoch_errors[-1]\n\n", "        #\n", "        # Print progress to the console\n", "        print(\n", "            f\"Epoch {str(epoch+1).zfill(int(np.log10(cfg.training.epochs))+1)}\"\n", "            f\"/{str(cfg.training.epochs)} took \"\n", "            f\"{str(np.round(time.time() - epoch_start_time, 2)).ljust(5, '0')} \"\n", "            f\"seconds. \\t\\tAverage epoch training error: \"\n", "            f\"{train_sign}\"\n", "            f\"{str(np.round(epoch_errors[-1], 10)).ljust(12, ' ')}\"\n", "        )\n", "    b = time.time()\n", "    print('\\nTraining took ' + str(np.round(b - a, 2)) + ' seconds.\\n\\n')"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}